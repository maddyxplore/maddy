{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nltk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-FS6jDuJVV"
      },
      "source": [
        "Nltk (natural Language Tool Kit) is the leading platform for building python programs to work with human language data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq8admhIuHr0"
      },
      "source": [
        "**Tokenizing**:\n",
        "                  It is a module which further classified into two types namely word trokenize and sent tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idfo0QVUsOxW"
      },
      "source": [
        "# NLTK \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence. showing off the stop words united states google filtration.\"\"\"\n",
        "print(\"####Tokenizing...\")\n",
        "print(word_tokenize(example_sent))\n",
        "print(sent_tokenize(example_sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqHyS7uvyxG"
      },
      "source": [
        "**Stop_Words**:\n",
        "                      These are the most commonly used words such as -- is, the, a, an, in, etc...using this we can filter out the useless data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEyfFdFstfKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d0d1f9-38f8-4c23-98c0-d2b25e154205"
      },
      "source": [
        "from nltk.corpus import stopwords, wordnet                                                                                                                                                                                                               # stop words\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence. showing off the stop words united states google filtration.\"\"\"\n",
        "print(\"#####stop_words\")\n",
        "print(stopwords.words('english'))\n",
        "print()\n",
        "print(\"####words without stop_words\")\n",
        "filtered_words = [words for words in word_tokenize(example_sent) if not words in stopwords.words('english') and words.isalpha()]\n",
        "print(filtered_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####stop_words\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "####words without stop_words\n",
            "['This', 'sample', 'sentence', 'showing', 'stop', 'words', 'united', 'states', 'google', 'filtration']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dAxniX_xGcC"
      },
      "source": [
        "**Stemming**:\n",
        "                It is a technique used to extract the base form of the words by removing affixes from them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3DLyHywzjUO"
      },
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, wordnet\n",
        "\n",
        "words = [\"code\",\"coding\",\"coders\",\"codery\"]\n",
        "ps = PorterStemmer() # stemming least ag\n",
        "ls = LancasterStemmer() # very agrressive\n",
        "rs = RegexpStemmer(\"ing\")\n",
        "print(\"####Porter Stemming...\")\n",
        "for word in words:\n",
        "    print(ps.stem(word))\n",
        "\n",
        "print()\n",
        "print(\"####Lancaster Stemming...\")\n",
        "for word in words:\n",
        "    print(ls.stem(word))\n",
        "\n",
        "print()\n",
        "print(\"#### Regexp Stemming...\")\n",
        "for word in words:\n",
        "    print(rs.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0D8qXsD0Ict"
      },
      "source": [
        "**Lemmantizing**:\n",
        "                              Lemmatization is similar to stemming but it brings context to the words.\n",
        "                              It refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olJ0N9MA0tPP"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "print()\n",
        "print(\"#### Lemmantizing\")\n",
        "print(lm.lemmatize(\"better\",\"a\"))\n",
        "print(lm.lemmatize(\"better\"))\n",
        "print(lm.lemmatize(\"corpora\"))\n",
        "print(lm.lemmatize(\"mice\"))\n",
        "print(lm.lemmatize(\"babies\"))\n",
        "print(lm.lemmatize(\"is\",\"v\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJo3Jl2W0-Ua"
      },
      "source": [
        "**POS_TAG**:\n",
        "                POS Tagging in NLTK is a process to mark up the words in text format for a particular part of a speech based on its definition and context.\n",
        "                Example:\n",
        "                NN - noun ,\n",
        "                VB - verb,\n",
        "                RB - Adverb,\n",
        "                PRP - pronoun,\n",
        "                JJ - adjective..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciMLAqh91bfT"
      },
      "source": [
        "print(\"#### POS Tagging\")    \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_words = [words for words in word_tokenize(example_sent) if not words in stopwords.words('english') and words.isalpha()] # isalpha to remove punctuation\n",
        "print(nltk.pos_tag(pos_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z6_bBIU49C2"
      },
      "source": [
        "**Chunking & Chinking**:\n",
        "                                      process of grouping similar words together based on the nature of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNwo4ppj5Jfq"
      },
      "source": [
        "postag = nltk.pos_tag(pos_words)\n",
        "\n",
        "print(\"#### chunking of words...\")\n",
        "\n",
        "# grammer \"NP: {}\"                                                                                                                                                         #Chunking\n",
        "grammer = \"\"\"Chunk: {<.*>+}\n",
        "\t\t\t\t    }<DT|VBG>{ \"\"\"                                                                                                                                                  #Chinking \n",
        "\n",
        "chunk = nltk.RegexpParser(grammer)\n",
        "res = chunk.parse(postag)\n",
        "\n",
        "res = nltk.ne_chunk(nltk.pos_tag(word_tokenize(example_sent)))\n",
        "print(res)\n",
        "#res.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "triJ-u7y6BMY"
      },
      "source": [
        "  **Wordnet**:\n",
        "                It is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muTgOkVp6Nc4"
      },
      "source": [
        "from nltk.corpus import wordnet as wordnet\n",
        "syns = wordnet.synsets(\"good\")\n",
        "s,a = [],[]\n",
        "print(syns[0].definition())                                                      # to print definition of the word\n",
        "for i in syns:                                                                              # lemma => good.n.01--> n --> noun (pos) \n",
        "    for l in i.lemmas():\n",
        "        s.append(l.name())\n",
        "        if l.antonyms():\n",
        "            a.append(l.antonyms()[0].name())\n",
        "\n",
        "print(\"Synonyms: \",set(s))\n",
        "print(\"Antonyms: \",set(a))\n",
        "\n",
        "# WuPalmer\n",
        "\n",
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"boat.n.01\")\n",
        "\n",
        "print(w1.wup_similarity(w2))\n",
        "\n",
        "w1 = wordnet.synset(\"mother.n.01\")\n",
        "w2 = wordnet.synset(\"child.n.01\")\n",
        "\n",
        "print(w1.wup_similarity(w2))\n",
        "\n",
        "w1 = wordnet.synset(\"mother.n.01\")\n",
        "w2 = wordnet.synset(\"father.n.01\")\n",
        "\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQJSrZXO6bB"
      },
      "source": [
        "**FreqDist**: (Frequency distribution)\n",
        "              It is an overview of all distinct values in some variable and the number of times they occur. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZcnPBp_PU2f"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "example_sent = \"\"\"This is a sample sentence. showing off the stop words united states google filtration. This sample sentence sample\"\"\"\n",
        "filtered_words = [words.lower() for words in word_tokenize(example_sent) if not words in stopwords.words('english') and words.isalpha()]\n",
        "print(filtered_words)\n",
        "fdist = FreqDist(filtered_words)\n",
        "print(fdist.most_common(3))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}